{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "yf.pdr_override() \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "import talib\n",
    "import pandas_ta\n",
    "from finta import TA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download and data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# List of ticker names\n",
    "ticker_names = ['USDEUR', 'USDJPY', 'USDGBP', 'USDCHF', 'USDCAD', 'USDAUD', 'USDNZD', 'USDSEK', 'USDNOK', 'USDDKK'\n",
    "                         , 'EURJPY', 'EURGBP', 'EURCHF', 'EURCAD', 'EURAUD', 'EURNZD', 'EURSEK', 'EURNOK', 'EURDKK']\n",
    "# Actual ticker symbols for Yahoo Finance data retrieval. These correspond to the ticker names\n",
    "ticker_list = ['EUR=X', 'JPY=X', 'GBP=X', 'CHF=X', 'CAD=X', 'AUD=X', 'NZD=X', 'SEK=X', 'NOK=X', 'DKK=X'\n",
    "                         , 'EURJPY=X', 'EURGBP=X', 'EURCHF=X', 'EURCAD=X', 'EURAUD=X', 'EURNZD=X', 'EURSEK=X', 'EURNOK=X', 'EURDKK=X']\n",
    "\n",
    "# Get today's date \n",
    "today = date.today()\n",
    "#today = '2023-02-15'\n",
    "\n",
    "# Initial start date \n",
    "start_date = datetime(2003, 6, 17)\n",
    "# List to store filenames of saved data\n",
    "files=[]\n",
    "\n",
    "# Function to save the processed data to a CSV file\n",
    "def SaveData(df, filename):\n",
    "    df.to_csv('./FX-Data/'+filename+'.csv')\n",
    "\n",
    "# Main function to fetch and process ticker data\n",
    "def getData(ticker, name):\n",
    "    \n",
    "    # Fetch data from Yahoo Finance for the given ticker and date range\n",
    "    data = pdr.get_data_yahoo(ticker, start=start_date, end=today)\n",
    "    \n",
    "    # Add classification column (Trend) based on Open and Adj Close prices\n",
    "    data = addClassifier(data)\n",
    "    # Compute various technical indicators for the data\n",
    "    data = addFeatures(data, 5)\n",
    "    #data = addExtraLags(data, [5,10])\n",
    "\n",
    "    # Specify the name of the data and append to a list\n",
    "    dataname= name\n",
    "    files.append(dataname)\n",
    "    \n",
    "    # Save the processed data to a CSV\n",
    "    SaveData(data, dataname)\n",
    "\n",
    "# Function to add various technical indicators to the dataframe\n",
    "def addFeatures(dataframe, lag):\n",
    "\n",
    "    # Calculate Parabolic SAR, Coppock Curve, and Typical Price and add to the dataframe\n",
    "    dataframe['Parabolic_SAR']=ta.trend.PSARIndicator(dataframe['High'],dataframe['Low'],dataframe['Close']).psar().shift(lag)\n",
    "    dataframe['Coppock_Curve']=TA.COPP(dataframe).shift(lag)\n",
    "    dataframe['Typical_Price']=TA.TP(dataframe).shift(lag)\n",
    "\n",
    "    # List of all technical indicator names\n",
    "    indicator1='RSI'\n",
    "    indicator2='SO'\n",
    "    indicator4='WI'\n",
    "    indicator5='ROC'\n",
    "    indicator6='EMA'\n",
    "    indicator7='CCI'\n",
    "    indicator8='BB_HB'\n",
    "    indicator9='BB_LB'\n",
    "    indicator10='BB_MAVG'\n",
    "    indicator12='DPO'\n",
    "    indicator13='ULCERINDEX'\n",
    "    indicator14='SMA'\n",
    "    indicator15='WMA'\n",
    "    indicator16='MOM'\n",
    "    indicator17='DX'\n",
    "    indicator18='TRIMA'\n",
    "    indicator19='AROON_DOWN'\n",
    "    indicator20='AROON_UP'\n",
    "    indicator21='AROONOSC'\n",
    "    indicator22='ADX'\n",
    "    indicator23='CMO'\n",
    "    indicator24='DEMA'\n",
    "    indicator25='MIDPOINT'\n",
    "    indicator26='MIDPRICE'\n",
    "    indicator27='NATR'\n",
    "    indicator28='TEMA'\n",
    "    indicator29='PSL'\n",
    "    indicator30='BIAS'\n",
    "    indicator31='RVI'\n",
    "    indicator32='LINREG'\n",
    "    indicator33='ACC_LOW'\n",
    "    indicator34='ACC_MID'\n",
    "    indicator35='ACC_UP'\n",
    "    indicator36='CHOP'\n",
    "    indicator37='RVGI'\n",
    "    indicator38='PERCENT_B'\n",
    "    indicator39='ATR'\n",
    "    indicator40='KC_UPPER'\n",
    "    indicator41='KC_LOWER'\n",
    "    indicator43='BBWIDTH'\n",
    "    indicator44='CHANDELIER_SHORT'\n",
    "    indicator45='CHANDELIER_LONG'\n",
    "    indicator48='HMA'\n",
    "    indicator49='KAMA'\n",
    "    indicator50='MI'\n",
    "    indicator51='MSD'\n",
    "    indicator52='TRIX'\n",
    "    indicator53='VORTEX_NEG'\n",
    "    indicator54='VORTEX_POS'\n",
    "    indicator100='MACD'\n",
    "    indicator101='PPO'\n",
    "    indicator104='APO'\n",
    "    indicator106='DO_UP'\n",
    "\n",
    "    # Calculate each technical indicator and add to the dataframe\n",
    "    dataframe[indicator1]=ta.momentum.rsi(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator2]=ta.momentum.stoch(dataframe['High'],dataframe['Low'],dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator14]=ta.trend.sma_indicator(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator4]=ta.momentum.williams_r(dataframe['High'],dataframe['Low'],dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator5]=ta.momentum.roc(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator6]=ta.trend.ema_indicator(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator7]=ta.trend.cci(dataframe['High'],dataframe['Low'],dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator8]=ta.volatility.BollingerBands(close=dataframe['Close']).bollinger_hband().shift(lag)\n",
    "    dataframe[indicator9]=ta.volatility.BollingerBands(close=dataframe['Close']).bollinger_lband().shift(lag)\n",
    "    dataframe[indicator10]=ta.volatility.BollingerBands(close=dataframe['Close']).bollinger_mavg().shift(lag)\n",
    "    dataframe[indicator12]=ta.trend.dpo(close=dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator13]=ta.volatility.ulcer_index(close=dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator15]=talib.WMA(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator16]=talib.MOM(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator17]=talib.DX(dataframe['High'],dataframe['Low'],dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator18]=talib.TRIMA(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator19]=talib.AROON(dataframe['High'], dataframe['Low'])[0].shift(lag)\n",
    "    dataframe[indicator20]=talib.AROON(dataframe['High'], dataframe['Low'])[1].shift(lag)\n",
    "    dataframe[indicator21]=talib.AROONOSC(dataframe['High'], dataframe['Low']).shift(lag)\n",
    "    dataframe[indicator22]=talib.ADX(dataframe['High'], dataframe['Low'], dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator23]=talib.CMO(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator24]=talib.DEMA(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator25]=talib.MIDPOINT(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator26]=talib.MIDPRICE(dataframe['High'], dataframe['Low']).shift(lag)\n",
    "    dataframe[indicator27]=talib.NATR(dataframe['High'], dataframe['Low'], dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator28]=talib.TEMA(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator29]=pandas_ta.psl(dataframe['Close'], dataframe['Open']).shift(lag)\n",
    "    dataframe[indicator30]=pandas_ta.bias(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator31]=pandas_ta.rvi(dataframe['Close'], dataframe['High'], dataframe['Low']).shift(lag)\n",
    "    dataframe[indicator32]=pandas_ta.linreg(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator33]=pandas_ta.accbands(dataframe['High'], dataframe['Low'], dataframe['Close']).iloc[:,0].shift(lag)\n",
    "    dataframe[indicator34]=pandas_ta.accbands(dataframe['High'], dataframe['Low'], dataframe['Close']).iloc[:,1].shift(lag)\n",
    "    dataframe[indicator35]=pandas_ta.accbands(dataframe['High'], dataframe['Low'], dataframe['Close']).iloc[:,2].shift(lag)\n",
    "    dataframe[indicator36]=pandas_ta.chop(dataframe['High'], dataframe['Low'], dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator37]=pandas_ta.rvgi(dataframe['Open'], dataframe['High'], dataframe['Low'], dataframe['Close']).iloc[:,0].shift(lag)\n",
    "    dataframe[indicator38]=TA.PERCENT_B(dataframe).shift(lag)\n",
    "    dataframe[indicator39]=TA.ATR(dataframe).shift(lag)\n",
    "    dataframe[indicator40]=TA.KC(dataframe).iloc[:,0].shift(lag)\n",
    "    dataframe[indicator41]=TA.KC(dataframe).iloc[:,1].shift(lag)\n",
    "    dataframe[indicator43]=TA.BBWIDTH(dataframe).shift(lag)\n",
    "    dataframe[indicator44]=TA.CHANDELIER(dataframe).iloc[:,0].shift(lag)\n",
    "    dataframe[indicator45]=TA.CHANDELIER(dataframe).iloc[:,1].shift(lag)\n",
    "    dataframe[indicator48]=TA.HMA(dataframe).shift(lag)\n",
    "    dataframe[indicator49]=TA.KAMA(dataframe).shift(lag)\n",
    "    dataframe[indicator50]=TA.MI(dataframe).shift(lag)\n",
    "    dataframe[indicator51]=TA.MSD(dataframe).shift(lag)\n",
    "    dataframe[indicator52]=TA.TRIX(dataframe).shift(lag)\n",
    "    dataframe[indicator53]=TA.VORTEX(dataframe).iloc[:,0].shift(lag)\n",
    "    dataframe[indicator54]=TA.VORTEX(dataframe).iloc[:,1].shift(lag)\n",
    "    dataframe[indicator100]=ta.trend.macd(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator101]=ta.momentum.ppo(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator104]=talib.APO(dataframe['Close']).shift(lag)\n",
    "    dataframe[indicator106]=TA.DO(dataframe).iloc[:,1].shift(lag)\n",
    "    \n",
    "    # Shift columns by a specified lag to prevent look-ahead bias\n",
    "    dataframe['Open'] = dataframe['Open'].shift(lag)\n",
    "    dataframe['Adj Close'] = dataframe['Adj Close'].shift(lag)\n",
    "    dataframe['Volume'] = dataframe['Volume'].shift(lag)\n",
    "    dataframe['High'] = dataframe['High'].shift(lag)\n",
    "    dataframe['Low'] = dataframe['Low'].shift(lag)\n",
    "\n",
    "    # Rearrange columns\n",
    "    dataframe = dataframe.drop('Close', axis=1)\n",
    "    dataframe = dataframe.drop('Volume', axis=1) \n",
    "    dataframe = dataframe.loc['2005-01-03':,:]\n",
    "    colnames = list(dataframe.columns)\n",
    "    colnames.remove('Trend')\n",
    "    dataframe = dataframe.loc[:,['Trend'] + colnames]\n",
    "\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Function to classify if the price trended uptick or downtick\n",
    "def addClassifier(dataframe):\n",
    "    ls_temp = []\n",
    "\n",
    "    # Determine trend direction based on Open and Adj Close prices\n",
    "    for ele in dataframe['Open']-dataframe['Adj Close']:\n",
    "        if ele<=0:\n",
    "            ls_temp.append(1)\n",
    "        else:\n",
    "            ls_temp.append(0)\n",
    "\n",
    "    dataframe['Trend'] = ls_temp\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Optional function to add extra lagged features to the dataframe\n",
    "def addExtraLags(dataframe, lags):\n",
    "    dataframe_colnames = dataframe.copy()\n",
    "    for lag in lags:\n",
    "        for col in dataframe_colnames.columns:\n",
    "            if col == 'Trend':\n",
    "                continue\n",
    "            name = col+'_'+str(lag)\n",
    "            dataframe[name] = dataframe[col].shift(lag-1) #minus one as the column is already of lag 1\n",
    "\n",
    "    max_lag = max(lags)\n",
    "    dataframe = dataframe.iloc[max_lag-1:,:]\n",
    "    return dataframe\n",
    "\n",
    "# For each ticker symbol, fetch the data and process it\n",
    "for name, ticker in zip(ticker_names, ticker_list):\n",
    "    getData(ticker, name)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4abf830dbe5492d4a2ce4ae1154053b5a6dd0334c53a1af2abe9dec18861041e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
